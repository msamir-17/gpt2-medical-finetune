{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOUtCGERi3c0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-rvEsl-DXAAS"
      },
      "outputs": [],
      "source": [
        "!pip install  transformers datasets accelerate huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76NTTrcHVNje"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qYA4TaqO5qpz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "G95nULXAjUAw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(\"hf://datasets/AnonymousSub/MedQuAD_47441_Question_Answer_Pairs/data/train-00000-of-00001-4401d00b2bdd1863.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mQQ8xjEX7hRX"
      },
      "outputs": [],
      "source": [
        "print(df.info())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1A2m2wqnr-IH"
      },
      "outputs": [],
      "source": [
        "# Normalize weird phrasing\n",
        "\n",
        "# Step 1: Select and rename\n",
        "df[\"prompt\"] = df[\"Questions\"].str.strip()\n",
        "df[\"response\"] = df[\"Answers\"].str.strip()\n",
        "\n",
        "df[\"prompt\"] = df[\"prompt\"].str.replace(r\"\\(are\\)\", \"is\", regex=True)\n",
        "# ‚úÖ Step 2: Drop real NaN first\n",
        "df = df.dropna(subset=[\"prompt\", \"response\"])\n",
        "\n",
        "# ‚úÖ Step 3: Drop if still any empty or 'None' as string\n",
        "df = df[(df[\"prompt\"].str.lower() != \"none\") & (df[\"response\"].str.lower() != \"none\")]\n",
        "df = df[(df[\"prompt\"].str.strip() != \"\") & (df[\"response\"].str.strip() != \"\")]\n",
        "\n",
        "# ‚úÖ Step 4: Now convert to string for safety\n",
        "df[\"prompt\"] = df[\"prompt\"].astype(str)\n",
        "df[\"response\"] = df[\"response\"].astype(str)\n",
        "\n",
        "# ‚úÖ Final format\n",
        "final_df = df[[\"prompt\", \"response\"]]\n",
        "\n",
        "# ‚úÖ Confirm\n",
        "print(final_df.sample(3))\n",
        "print(f\"\\n‚úÖ Cleaned dataset ready with {len(final_df)} samples.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TTBMeUopsZ64"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "dff = Dataset.from_pandas(final_df)\n",
        "print(dff[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9MIUA5Ujtl4F"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnQskWqMvfp_"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    # Combine prompt + response for each example in batch\n",
        "    full_texts = [p + \" \" + r for p, r in zip(examples[\"prompt\"], examples[\"response\"])]\n",
        "\n",
        "    # Tokenize all at once (batch)\n",
        "    tokenized = tokenizer(\n",
        "        full_texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # GPT-style: labels = input_ids\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ByRZCUXsww2C"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))  # Token embeddings adjust kiye\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9pc4R72n0gXy"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-medical-finetuned\",       # Output directory\n",
        "    eval_strategy=\"no\",                      # No evaluation\n",
        "    per_device_train_batch_size=4,              # Batch size per device\n",
        "    num_train_epochs=1,                         # Number of training epochs\n",
        "    save_steps=500,                             # Save model every 500 steps\n",
        "    # eval_steps=500,                           # Commented, kyunki eval_strategy=\"no\"\n",
        "    logging_steps=100,                          # Log every 100 steps\n",
        "    warmup_steps=100,                           # Learning rate warmup\n",
        "    weight_decay=0.01,                          # Regularization\n",
        "    save_total_limit=2,                         # Max saved checkpoints\n",
        "    logging_dir=\"./logs\",                       # Logging directory\n",
        "    fp16=True,                                  # Use mixed precision (if GPU supports)\n",
        "    report_to=\"none\"                            # Disable external reporting\n",
        ")\n",
        "\n",
        "tokenized_data = dff.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSGBKfbd3ws8"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args= training_args,\n",
        "    train_dataset=tokenized_data,\n",
        "    tokenizer = tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3AU8j7zM7hkW"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jcOksr-8-722"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained('gpt2-medical-finetuned1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gtTtWtSVbVZ9"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"gpt2-medical-finetuned1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niwGKcwdbY7r"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load fine-tuned model\n",
        "medical_bot = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"gpt2-medical-finetuned1\",\n",
        "    tokenizer=\"gpt2-medical-finetuned1\",\n",
        "    pad_token_id=50256  # GPT-2 ka eos_token_id\n",
        ")\n",
        "\n",
        "# Test with prompt\n",
        "prompt = \"What is the best treatment for diabetes?\"\n",
        "response = medical_bot(\n",
        "    prompt,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.7\n",
        ")[0]['generated_text']\n",
        "print(\"üîç Model Response:\\n\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foLkbiSIb6Nu"
      },
      "source": [
        "**üß† 1. Load Fine-Tuned Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtVK63lubjO8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2-medical-finetuned1')\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2-medical-finetuned1')\n",
        "\n",
        "# Test function\n",
        "def test_model(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test with prompt\n",
        "prompt = \"What is the best treatment for diabetes?\"\n",
        "print(\"üîç Model Response:\\n\", test_model(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBzGSnLecBMm"
      },
      "source": [
        "**2. Define Test Function**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc-7tELNcWI9"
      },
      "source": [
        "**3. Run Test (Give Prompt!)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C9IQvjYcdIV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3xv-INVELTq"
      },
      "outputs": [],
      "source": [
        "test_prompts = [\n",
        "    \"What is the first sign of Hepatitis B?\",\n",
        "    \"How can I manage high blood pressure?\",\n",
        "    \"Symptoms of type 2 diabetes?\",\n",
        "    \"Is Hepatitis B contagious?\"\n",
        "]\n",
        "\n",
        "for p in test_prompts:\n",
        "    print(f\"\\nüß™ Prompt: {p}\")\n",
        "    print(\"üîç Response:\", test_model(p))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHmDdNTsYXIs"
      },
      "outputs": [],
      "source": [
        "!zip -r model.zip gpt2-medical-finetuned1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZXBTJ8LiKsm"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"metadata\": {\n",
        "    \"widgets\": {\n",
        "      \"state\": {}\n",
        "    }\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYrfBt8wjEmO"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/finetuned models/model.zip\"\n",
        "extract_path = \"/content/model\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Check contents\n",
        "os.listdir(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_folder\n",
        "\n",
        "# Update repo_id yahan:\n",
        "repo_id = \"samirk10/fine-tune\"  # replace with your actual repo ID\n",
        "local_model_path = \"model/gpt2-medical-finetuned1\"  # tumhara fine-tuned model folder\n",
        "\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=local_model_path,\n",
        "    path_in_repo=\".\",  # root pe upload\n",
        "    commit_message=\"Pushing fine-tuned GPT-2 medical model üöÄ\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "eD6xfkNfv5_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XLTVc4elyJVr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}