# ğŸ§  GPT-2 Medical Fine-Tuned Model

This repository contains a fine-tuned version of OpenAI's GPT-2 model, trained on medical question-answer pairs from the [MedQuAD](https://www.nlm.nih.gov/dimrc/medquad.html) dataset.  
The goal is to build an AI assistant capable of answering common medical queries in a human-like, informative manner.

ğŸ”— Try the model here: [Hugging Face - samirk10/fine-tune](https://huggingface.co/samirk10/fine-tune)

---

## ğŸ“¦ Repository Contents

- `Fine_tune.ipynb`: Google Colab notebook for fine-tuning GPT-2 on medical Q&A.
- `gpt2-medical-finetuned2/`: Folder containing the trained model, tokenizer, and config files.
- `README.md`: Project overview and usage instructions.

---

## ğŸ“Š Dataset

- **Source**: MedQuAD (NIH)
- **Size**: ~47,000 Q&A pairs
- **Format**:
  - `prompt`: Medical question
  - `response`: Medical answer

---

## ğŸ‹ï¸ Model Training Details

- **Base Model**: `gpt2`
- **Library**: ğŸ¤— Hugging Face Transformers (`v4.53.1`)
- **Platform**: Google Colab / Kaggle (T4 / P100 GPU)
- **Epochs**: 1
- **Max Token Length**: 512
- **Mixed Precision**: `fp16`

---

## ğŸ› ï¸ Key Issues & Solutions

| Issue | Fix |
|------|-----|
| âŒ Older Transformers version broke `Trainer` / `evaluation_strategy` | âœ… Upgraded to `v4.53.1` |
| âŒ `NaN` or `None` responses in dataset | âœ… Cleaned before training |
| âŒ Colab GPU disconnection | âœ… Switched to Kaggle |
| âŒ Model download problems | âœ… Used zip + Google Drive/Colab |

---

## ğŸš€ Inference Example

```python
from transformers import pipeline

medical_bot = pipeline(
    "text-generation",
    model="samirk10/fine-tune",  # Hugging Face Model
    tokenizer="samirk10/fine-tune",
    pad_token_id=50256
)

prompt = "What is the best treatment for diabetes?"
response = medical_bot(prompt, max_new_tokens=100)[0]['generated_text']
print(response)
