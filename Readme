# 🧠 GPT-2 Medical Fine-Tuned Model

This repository contains a fine-tuned version of OpenAI's GPT-2 model, trained on medical question-answer pairs from the [MedQuAD](https://www.nlm.nih.gov/dimrc/medquad.html) dataset.  
The goal is to build an AI assistant capable of answering common medical queries in a human-like, informative manner.

🔗 Try the model here: [Hugging Face - samirk10/fine-tune](https://huggingface.co/samirk10/fine-tune)

---

## 📦 Repository Contents

- `Fine_tune.ipynb`: Google Colab notebook for fine-tuning GPT-2 on medical Q&A.
- `gpt2-medical-finetuned2/`: Folder containing the trained model, tokenizer, and config files.
- `README.md`: Project overview and usage instructions.

---

## 📊 Dataset

- **Source**: MedQuAD (NIH)
- **Size**: ~47,000 Q&A pairs
- **Format**:
  - `prompt`: Medical question
  - `response`: Medical answer

---

## 🏋️ Model Training Details

- **Base Model**: `gpt2`
- **Library**: 🤗 Hugging Face Transformers (`v4.53.1`)
- **Platform**: Google Colab / Kaggle (T4 / P100 GPU)
- **Epochs**: 1
- **Max Token Length**: 512
- **Mixed Precision**: `fp16`

---

## 🛠️ Key Issues & Solutions

| Issue | Fix |
|------|-----|
| ❌ Older Transformers version broke `Trainer` / `evaluation_strategy` | ✅ Upgraded to `v4.53.1` |
| ❌ `NaN` or `None` responses in dataset | ✅ Cleaned before training |
| ❌ Colab GPU disconnection | ✅ Switched to Kaggle |
| ❌ Model download problems | ✅ Used zip + Google Drive/Colab |

---

## 🚀 Inference Example

```python
from transformers import pipeline

medical_bot = pipeline(
    "text-generation",
    model="samirk10/fine-tune",  # Hugging Face Model
    tokenizer="samirk10/fine-tune",
    pad_token_id=50256
)

prompt = "What is the best treatment for diabetes?"
response = medical_bot(prompt, max_new_tokens=100)[0]['generated_text']
print(response)
